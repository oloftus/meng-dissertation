function [y1] = network(x1)
%NET neural network simulation function.
%
% Generated by Neural Network Toolbox function genFunction, 14-Mar-2016 14:27:41.
% 
% [y1] = net(x1) takes these arguments:
%   x = 3xQ matrix, input #1
% and returns:
%   y = 2xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

  % ===== NEURAL NETWORK CONSTANTS =====
  
  % Input 1
  x1_step1_xoffset = [0;0;0];
  x1_step1_gain = [2;2;2];
  x1_step1_ymin = -1;
  
  % Layer 1
  b1 = [-2.5787663253473755987;1.381783162673687837;-0.18480000000000001981;1.381783162673687837;2.209166325347375448];
  IW1_1 = [1.0116553791170532772 -1.6097595204129730995 -1.5483398215089168026;-1.3074952921691400665 -1.7582387189514439019 0.79727020262425740693;-1.5454428467461500674 0.20473124491963068383 1.7927800581513309019;0.97435588854304988615 2.0492236568597155255 -0.52658145993508198579;0.66302605637294254048 1.5080639806585864893 -1.9906898589127506316];
  
  % Layer 2
  b2 = [1.8896215260457007723;-0.78101076302285044051;-0.042000000000000002609;-0.78101076302285044051;1.8896215260457007723];
  LW2_1 = [-0.88528022333866063232 1.0439227826791901421 1.3510507821137036633 0.15512438117261861947 0.08541627609674526389;0.57131223274853648597 -1.1340587176869572072 -0.63585038656417880443 1.0350351498064138678 1.1132379523329425375;-1.2983555826994868099 -0.74496657382389697144 0.21573850911351025172 0.82115718072390719851 -0.92528012383294500065;-0.061950484740520006532 0.66282322921265102167 0.99334144904099985762 1.1260689264692740341 0.99193100052688187862;-0.052768576487270915654 -1.3276379296734932556 1.2684114187783670502 -1.1229958639372092932 -0.26504556596739270447];
  
  % Layer 3
  b3 = [0.57517238499507838956;0.78114036827467525637];
  LW3_2 = [-0.018282837544591742929 -1.0151550807383271646 0.64945871775375996204 -0.64586489353091203203 0.35954421036593436911;-0.29410383629631803304 -0.21893429763196664561 0.12583638549199055445 0.7107409538895591572 -0.77260075178066467405];
  
  % ===== SIMULATION ========
  
  % Dimensions
  Q = size(x1,2); % samples
  
  % Input 1
  xp1 = mapminmax_apply(x1,x1_step1_gain,x1_step1_xoffset,x1_step1_ymin);
  
  % Layer 1
  a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);
  
  % Layer 2
  a2 = tansig_apply(repmat(b2,1,Q) + LW2_1*a1);
  
  % Layer 3
  a3 = softmax_apply(repmat(b3,1,Q) + LW3_2*a2);
  
  % Output 1
  y1 = a3;
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings_gain,settings_xoffset,settings_ymin)
  y = bsxfun(@minus,x,settings_xoffset);
  y = bsxfun(@times,y,settings_gain);
  y = bsxfun(@plus,y,settings_ymin);
end

% Competitive Soft Transfer Function
function a = softmax_apply(n)
  nmax = max(n,[],1);
  n = bsxfun(@minus,n,nmax);
  numer = exp(n);
  denom = sum(numer,1); 
  denom(denom == 0) = 1;
  a = bsxfun(@rdivide,numer,denom);
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n)
  a = 2 ./ (1 + exp(-2*n)) - 1;
end
